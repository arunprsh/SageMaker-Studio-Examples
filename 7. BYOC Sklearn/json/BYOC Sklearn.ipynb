{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Scikit-Learn Model using SageMaker Container Mode\n",
    "#### Bring Your Own Container (BYOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Train Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train\n"
     ]
    }
   ],
   "source": [
    "%%file train\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define paths for Model Training inside Container.\n",
    "INPUT_PATH = '/opt/ml/input/data'\n",
    "OUTPUT_PATH = '/opt/ml/output'\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "PARAM_PATH = '/opt/ml/input/config/hyperparameters.json'\n",
    "\n",
    "# Training data sitting in S3 will be copied to this location during training when used with File MODE.\n",
    "TRAIN_DATA_PATH = f'{INPUT_PATH}/train'\n",
    "TEST_DATA_PATH = f'{INPUT_PATH}/test'\n",
    "\n",
    "def train():\n",
    "    print(\"------- [STARTING TRAINING] -------\")\n",
    "    train_df = pd.read_csv(os.path.join(TRAIN_DATA_PATH, 'train.csv'), names=['class', 'bmi', 'diastolic_bp_change', 'systolic_bp_change', 'respiratory_rate'])\n",
    "    train_df.head()\n",
    "    X_train = train_df[['bmi', 'diastolic_bp_change', 'systolic_bp_change', 'respiratory_rate']]\n",
    "    y_train = train_df['class']\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    # Save the trained Model inside the Container\n",
    "    with open(os.path.join(MODEL_PATH, 'model.pkl'), 'wb') as out:\n",
    "        pickle.dump(knn, out)\n",
    "    print(\"------- [TRAINING COMPLETE!] -------\")\n",
    "    \n",
    "    print(\"------- [STARTING EVALUATION] -------\")\n",
    "    test_df = pd.read_csv(os.path.join(TEST_DATA_PATH, 'test.csv'), names=['class', 'bmi', 'diastolic_bp_change', 'systolic_bp_change', 'respiratory_rate'])\n",
    "    X_test = train_df[['bmi', 'diastolic_bp_change', 'systolic_bp_change', 'respiratory_rate']]\n",
    "    y_test = train_df['class']\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    print('Accuracy = {:.2f}%'.format(acc * 100))\n",
    "    print(\"------- [EVALUATION DONE!] -------\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Serve Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serve\n"
     ]
    }
   ],
   "source": [
    "%%file serve\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from flask import Flask, Response, request\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "MODEL_PATH = '/opt/ml/model'\n",
    "\n",
    "# Singleton Class for holding the Model\n",
    "class Predictor:\n",
    "    model = None\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls):\n",
    "        print('[LOADING MODEL]')\n",
    "        if cls.model is None:\n",
    "            with open(os.path.join(MODEL_PATH, 'model.pkl'), 'rb') as file_:\n",
    "                cls.model = pickle.load(file_)\n",
    "        print('MODEL LOADED!')\n",
    "        return cls.model\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls, X):\n",
    "        clf = cls.load_model()\n",
    "        return clf.predict(X)\n",
    "\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    print('[HEALTH CHECK]')\n",
    "    model = Predictor.load_model()\n",
    "    status = 200\n",
    "    if model is None:\n",
    "        status = 404\n",
    "    return Response(response={\"HEALTH CHECK\": \"OK\"}, status=status, mimetype='application/json')\n",
    "\n",
    "@app.route('/invocations', methods=['POST'])\n",
    "def invoke():\n",
    "    data = None\n",
    "\n",
    "    # Transform Payload in CSV to Pandas DataFrame.\n",
    "    if request.content_type == 'application/JSON':\n",
    "        data = request.data.decode('utf-8')\n",
    "        data = StringIO(data)\n",
    "        data = pd.read_csv(data, header=None)\n",
    "        # TODO add logic to read incoming payload in JSON\n",
    "        # TODO\n",
    "    else:\n",
    "        return flask.Response(response='This Predictor only supports JSON data', status=415, mimetype='text/plain')\n",
    "\n",
    "    logging.info('Invoked with {} records'.format(data.shape[0]))\n",
    "    \n",
    "    predictions = Predictor.predict(data)\n",
    "\n",
    "    # Convert from numpy back to JSON\n",
    "    out = StringIO()\n",
    "    pd.DataFrame({'results': predictions}).to_csv(out, header=False, index=False)\n",
    "    result = out.getvalue()\n",
    "\n",
    "    return Response(response=result, status=200, mimetype='application/JSON')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a Docker Image and Push to ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Build the docker image and push to ECR and have the image URI handy for the next steps.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  41.47kB\n",
      "Step 1/8 : FROM python:3.7\n",
      "3.7: Pulling from library/python\n",
      "\n",
      "\u001b[1B22b926a1: Pulling fs layer \n",
      "\u001b[1B09ae8373: Pulling fs layer \n",
      "\u001b[1Be3daef68: Pulling fs layer \n",
      "\u001b[1B244fe254: Pulling fs layer \n",
      "\u001b[1Bbed20e89: Pulling fs layer \n",
      "\u001b[1B03a5a371: Pulling fs layer \n",
      "\u001b[1B42025f7f: Pulling fs layer \n",
      "\u001b[1B766649b2: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:ed9c421e77eab107bb9ad431fa734f5407ebf8b241dcab2c22a7cfc76e9139f0[6A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for python:3.7\n",
      " ---> ac9dead5ba6f\n",
      "Step 2/8 : COPY requirements.txt ./\n",
      " ---> 941295b8dcda\n",
      "Step 3/8 : RUN pip install --no-cache-dir -r requirements.txt\n",
      " ---> Running in 21f032bbbb9c\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "Collecting flask\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting numpy>=1.16.5\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "Installing collected packages: six, numpy, MarkupSafe, Werkzeug, threadpoolctl, scipy, pytz, python-dateutil, joblib, Jinja2, itsdangerous, click, scikit-learn, pandas, flask\n",
      "Successfully installed Jinja2-2.11.3 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 joblib-1.0.1 numpy-1.20.1 pandas-1.2.3 python-dateutil-2.8.1 pytz-2021.1 scikit-learn-0.24.1 scipy-1.6.2 six-1.15.0 threadpoolctl-2.1.0\n",
      "Removing intermediate container 21f032bbbb9c\n",
      " ---> 932833551dc0\n",
      "Step 4/8 : COPY train /usr/local/bin\n",
      " ---> 6390e565dfe6\n",
      "Step 5/8 : RUN chmod +x /usr/local/bin/train\n",
      " ---> Running in b7719cc9aeb3\n",
      "Removing intermediate container b7719cc9aeb3\n",
      " ---> 4a1b26e9a43d\n",
      "Step 6/8 : COPY serve /usr/local/bin\n",
      " ---> 2c5322eba202\n",
      "Step 7/8 : RUN chmod +x /usr/local/bin/serve\n",
      " ---> Running in 36fd1016b717\n",
      "Removing intermediate container 36fd1016b717\n",
      " ---> 2c4f302c6a81\n",
      "Step 8/8 : EXPOSE 8080\n",
      " ---> Running in 6d97d750786d\n",
      "Removing intermediate container 6d97d750786d\n",
      " ---> cc4f9c60a92e\n",
      "Successfully built cc4f9c60a92e\n",
      "Successfully tagged sagemaker-byoc-sklearn:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t sagemaker-byoc-sklearn -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push local Docker image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container Name:  sagemaker-byoc-sklearn\n",
      "Account:  892313895307\n",
      "Region: us-east-1\n",
      "ECR Repository:  892313895307.dkr.ecr.us-east-1.amazonaws.com\n",
      "ECR Image URI:  892313895307.dkr.ecr.us-east-1.amazonaws.com/sagemaker-byoc-sklearn:latest\n",
      "Login Succeeded\n",
      "The push refers to repository [892313895307.dkr.ecr.us-east-1.amazonaws.com/sagemaker-byoc-sklearn]\n",
      "f7f117824127: Preparing\n",
      "a00d2ba51cb5: Preparing\n",
      "b1523eeca3f0: Preparing\n",
      "8e3105203750: Preparing\n",
      "6cb5a55ce424: Preparing\n",
      "f078a683635a: Preparing\n",
      "0b18c63fe124: Preparing\n",
      "abb35d8edc01: Preparing\n",
      "2cdb72475c99: Preparing\n",
      "04d1717d0e01: Preparing\n",
      "dacb447ffe30: Preparing\n",
      "bde301416dd2: Preparing\n",
      "81496d8c72c2: Preparing\n",
      "644448d6e877: Preparing\n",
      "0e41e5bdb921: Preparing\n",
      "04d1717d0e01: Waiting\n",
      "dacb447ffe30: Waiting\n",
      "f078a683635a: Waiting\n",
      "bde301416dd2: Waiting\n",
      "81496d8c72c2: Waiting\n",
      "0b18c63fe124: Waiting\n",
      "644448d6e877: Waiting\n",
      "0e41e5bdb921: Waiting\n",
      "abb35d8edc01: Waiting\n",
      "2cdb72475c99: Waiting\n",
      "a00d2ba51cb5: Pushed\n",
      "b1523eeca3f0: Pushed\n",
      "8e3105203750: Pushed\n",
      "f7f117824127: Pushed\n",
      "f078a683635a: Pushed\n",
      "abb35d8edc01: Pushed\n",
      "0b18c63fe124: Pushed\n",
      "04d1717d0e01: Pushed\n",
      "2cdb72475c99: Pushed\n",
      "81496d8c72c2: Pushed\n",
      "644448d6e877: Pushed\n",
      "bde301416dd2: Pushed\n",
      "0e41e5bdb921: Pushed\n",
      "6cb5a55ce424: Pushed\n",
      "dacb447ffe30: Pushed\n",
      "latest: digest: sha256:e23090588f5de4d50707c0fe9ba02e63f6eb1937462ca378c175a2a15ef44581 size: 3467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Specify a name to your custom container\n",
    "container_name=sagemaker-byoc-sklearn\n",
    "echo \"Container Name: \" ${container_name}\n",
    "\n",
    "# Retreive AWS account ID\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the AWS region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "echo \"Account: \" ${account}\n",
    "echo \"Region: \"${region}\n",
    "\n",
    "repository=\"${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "echo \"ECR Repository: \" ${repository}\n",
    "\n",
    "image=\"${account}.dkr.ecr.${region}.amazonaws.com/${container_name}:latest\"\n",
    "echo \"ECR Image URI: \" ${image}\n",
    "\n",
    "# If the ECR repository does not exist, create it.\n",
    "aws ecr describe-repositories --repository-names ${container_name} > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "aws ecr create-repository --repository-name ${container_name} > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${repository}\n",
    "\n",
    "# Tag the local image with ECR image name\n",
    "docker tag ${container_name} ${image}\n",
    "\n",
    "# Finally, push the local docker image to ECR with the full ECR image name\n",
    "docker push ${image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train your Custom Sklearn Model using SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "import pandas as pd\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "account = session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = session.boto_session.region_name\n",
    "image_name = 'sagemaker-byoc-sklearn'\n",
    "image_uri = f'{account}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (using SageMaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = '../.././DATA'\n",
    "\n",
    "train_data_s3_pointer = session.upload_data(f'{WORK_DIRECTORY}/train', key_prefix='byoc-sklearn/train')\n",
    "test_data_s3_pointer = session.upload_data(f'{WORK_DIRECTORY}/test', key_prefix='byoc-sklearn/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-892313895307/byoc-sklearn/train'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_s3_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-892313895307/byoc-sklearn/test'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_s3_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=session  # ensure the session is set to session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-26 17:48:39 Starting - Starting the training job...\n",
      "2021-03-26 17:48:41 Starting - Launching requested ML instancesProfilerReport-1616780918: InProgress\n",
      ".........\n",
      "2021-03-26 17:50:33 Starting - Preparing the instances for training...\n",
      "2021-03-26 17:51:07 Downloading - Downloading input data...\n",
      "2021-03-26 17:51:33 Training - Downloading the training image...\n",
      "2021-03-26 17:52:05 Uploading - Uploading generated training model\u001b[34m------- [STARTING TRAINING] -------\u001b[0m\n",
      "\u001b[34m------- [TRAINING COMPLETE!] -------\u001b[0m\n",
      "\u001b[34m------- [STARTING EVALUATION] -------\u001b[0m\n",
      "\u001b[34mAccuracy = 82.42%\u001b[0m\n",
      "\u001b[34m------- [EVALUATION DONE!] -------\u001b[0m\n",
      "\n",
      "2021-03-26 17:52:33 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n"
     ]
    }
   ],
   "source": [
    "model.fit({'train': train_data_s3_pointer, 'test': test_data_s3_pointer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Trained Model as SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "json_serializer = JSONSerializer()\n",
    "predictor = model.deploy(1, 'ml.m5.xlarge', \n",
    "                         endpoint_name='emr-byoc-sklearn', \n",
    "                         serializer=json_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Time Inference using Deployed Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.././DATA/test/test.csv', header=None)\n",
    "test_df = df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.733637</td>\n",
       "      <td>0.347981</td>\n",
       "      <td>0.228029</td>\n",
       "      <td>0.162324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4\n",
       "1608  0.733637  0.347981  0.228029  0.162324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.drop(test_df.columns[[0]], axis=1, inplace=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73363737, 0.3479813 , 0.22802851, 0.16232361]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test_df.values).decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
