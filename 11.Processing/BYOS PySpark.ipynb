{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing (PySpark) Example - Script Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create SparkML Pre-process Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Your Own Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-features.html\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --------------------------------- CONSTRUCT ---------------------------------\n",
    "    spark = SparkSession.builder.appName(\"PySparkJob\").getOrCreate()\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help=\"s3 input bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\",\n",
    "                                                      \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "    # --------------------------------- CONSTRUCT ---------------------------------\n",
    "    \n",
    "    schema = StructType([StructField('id', IntegerType(), True), \n",
    "                     StructField('name', StringType(), True),\n",
    "                     StructField('age', IntegerType(), True),\n",
    "                     StructField('sex', StringType(), True),\n",
    "                     StructField('weight', DoubleType(), True),\n",
    "                     StructField('eye_color', StringType(), True)\n",
    "                    ])\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    # without header (IMPORTANT)\n",
    "    df = spark.read.csv(('s3a://' + os.path.join(args.s3_input_bucket, \n",
    "                                                 args.s3_input_key_prefix,\n",
    "                                                 'raw.csv')), header=False, schema=schema)\n",
    "    sex_indexer = StringIndexer(inputCol='sex', outputCol='indexed_sex')\n",
    "    sex_encoder = OneHotEncoder(inputCol='indexed_sex', outputCol='sex_vector')\n",
    "    eye_color_indexer = StringIndexer(inputCol='eye_color', outputCol='indexed_eye_color')\n",
    "    eye_color_encoder = OneHotEncoder(inputCol='indexed_eye_color', outputCol='eye_color_vector')\n",
    "    assembler = VectorAssembler(inputCols=['age',\n",
    "                                           'weight',\n",
    "                                           'sex_vector',\n",
    "                                           'eye_color_vector'], \n",
    "                                outputCol='features')\n",
    "    scaler = MinMaxScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "    \n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, eye_color_indexer, eye_color_encoder, assembler, scaler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    df = model.transform(df)\n",
    "    \n",
    "    age_udf = udf(lambda x: x[0].item(), DoubleType())\n",
    "    weight_udf = udf(lambda x: x[1].item(), DoubleType())\n",
    "    sex_udf = udf(lambda x: x[2].item(), DoubleType())\n",
    "    blue_eye_udf = udf(lambda x: x[3].item(), DoubleType())\n",
    "    black_eye_udf = udf(lambda x: x[3].item(), DoubleType())\n",
    "    \n",
    "    df = df.select(age_udf('scaledFeatures').alias('age'), \n",
    "               weight_udf('scaledFeatures').alias('weight'),\n",
    "               sex_udf('scaledFeatures').alias('sex'),\n",
    "               blue_eye_udf('scaledFeatures').alias('is_blue_eye'),\n",
    "               black_eye_udf('scaledFeatures').alias('is_black_eye'),\n",
    "              )\n",
    "    df.show()\n",
    "    \n",
    "    df.write.format('csv') \\\n",
    "        .option('header', True) \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('sep', ',') \\\n",
    "        .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker Processing Job (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from time import gmtime, strftime\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "prefix = 'spark-preprocess-jobs/' + timestamp_prefix\n",
    "input_prefix = prefix + '/raw'\n",
    "input_preprocessed_prefix = prefix + '/transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-892313895307/spark-preprocess-jobs/2020-12-10-00-22-37/raw/raw.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading the training data to S3\n",
    "sagemaker_session.upload_data(path='./DATA/raw.csv', bucket=bucket, key_prefix=input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name spark-preprocessor-2020-12-10-00-22-39-869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-preprocessor-2020-12-10-00-22-39-869\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/spark-preprocessor-2020-12-10-00-22-39-869/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-892313895307/spark-preprocess-jobs/2020-12-10-00-22-37/spark_event_logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      ".......................................................................!"
     ]
    }
   ],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='spark-preprocessor',\n",
    "                                    framework_version='2.4',\n",
    "                                    role=role,\n",
    "                                    instance_count=2,\n",
    "                                    instance_type='ml.r5.xlarge',\n",
    "                                    max_runtime_in_seconds=1200)\n",
    "\n",
    "spark_processor.run(submit_app='preprocess.py',\n",
    "                    arguments=[\"--s3_input_bucket\", bucket,\n",
    "                                \"--s3_input_key_prefix\", input_prefix,\n",
    "                                \"--s3_output_bucket\", bucket,\n",
    "                                \"--s3_output_key_prefix\", input_preprocessed_prefix],\n",
    "                    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "                    logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_path = f's3://{bucket}/{input_preprocessed_prefix}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-892313895307/spark-preprocess-jobs/2020-12-10-00-22-37/transformed/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 187 Bytes/187 Bytes (3.6 KiB/s) with 2 file(s) remaining\r",
      "download: s3://sagemaker-us-east-1-892313895307/spark-preprocess-jobs/2020-12-10-00-22-37/transformed/_SUCCESS to DATA/_SUCCESS\r\n",
      "Completed 187 Bytes/187 Bytes (3.6 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-east-1-892313895307/spark-preprocess-jobs/2020-12-10-00-22-37/transformed/part-00000-6ee6f5b3-0fae-4510-ab77-b674c70a98bc-c000.csv to DATA/part-00000-6ee6f5b3-0fae-4510-ab77-b674c70a98bc-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {output_s3_path} ./DATA/ --recursive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> Use the downloaded part file above to create the dataframe below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>sex</th>\n",
       "      <th>is_blue_eye</th>\n",
       "      <th>is_black_eye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.773279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.567684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.765063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    weight  sex  is_blue_eye  is_black_eye\n",
       "0  0.56  0.773279  0.0          0.0           0.0\n",
       "1  0.60  0.567684  1.0          1.0           1.0\n",
       "2  0.00  0.000000  1.0          0.0           0.0\n",
       "3  1.00  1.000000  0.0          1.0           1.0\n",
       "4  0.36  0.765063  1.0          0.0           0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df = pd.read_csv('./DATA/part-00000-6ee6f5b3-0fae-4510-ab77-b674c70a98bc-c000.csv')\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the History Server to access Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.start_history_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> Use the URL above to access the Spark UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
